{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import Metric, WassersteinApproximation, StructuralDissimilarity, L2Metric, LinfMetric, LpMetric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models\\\\model_v1.model')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        'mnist',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    ),\n",
    "    batch_size=50,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_benign_examples(model, dataloader, count):\n",
    "    counter = 0\n",
    "    benign_examples = torch.zeros(count, 1, dataloader.dataset[0][0].shape[1], dataloader.dataset[0][0].shape[2])\n",
    "    benign_labels = torch.zeros(count)\n",
    "    for examples, labels in dataloader:\n",
    "        preds = model(examples)\n",
    "        match = (torch.argmax(preds, dim=1) == labels)\n",
    "        for idx, foo in enumerate(match):\n",
    "            if foo:\n",
    "                benign_examples[counter] = examples[idx]\n",
    "                benign_labels[counter] = labels[idx]\n",
    "                counter += 1\n",
    "            if counter >= count:\n",
    "                break\n",
    "        if counter >= count:\n",
    "            break\n",
    "    return benign_examples, benign_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 20\n",
    "\n",
    "benign, labels = get_benign_examples(model, train_loader, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cw_batch(model: nn.Module, benign_examples: torch.Tensor, labels: torch.Tensor, c_lambda: float, metric: Metric, special_init = False) -> torch.Tensor:\n",
    "    if special_init:\n",
    "        adversarial_examples = benign_examples\n",
    "    else:\n",
    "        adversarial_examples = 0.5 * torch.ones(benign_examples.shape) + 0.3 * (2 * torch.rand(benign_examples.shape) - 1)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    step_size = 1e-2\n",
    "    for i in range(100):\n",
    "        adversarial_examples.requires_grad = True\n",
    "        if adversarial_examples.grad is not None:\n",
    "            adversarial_examples.grad.zero_()\n",
    "        benign_examples.requires_grad = True\n",
    "        if benign_examples.grad is not None:\n",
    "            benign_examples.grad.zero_()\n",
    "        metrics = metric(benign_examples, adversarial_examples)\n",
    "        # if (i + 1) % 10 == 0:\n",
    "        #     print(metrics)\n",
    "        loss = metrics.sum() - c_lambda * loss_fn(model(adversarial_examples), torch.tensor(labels, dtype=torch.long))\n",
    "        \n",
    "        loss.backward()\n",
    "        # if (i + 1) % 10 == 0:\n",
    "        #     print(adversarial_examples.grad)\n",
    "        adversarial_examples = (adversarial_examples - step_size * adversarial_examples.grad.apply_(lambda x: 1 if x >= 0 else -1)).detach()\n",
    "        # ex = adversarial_examples[0].detach().reshape(28, 28)\n",
    "        # if (i + 1) % 10 == 0:   \n",
    "        #     plt.imshow(ex, cmap='gray', vmin=0, vmax=1)\n",
    "        #     plt.show()\n",
    "    return adversarial_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [1000, 100, 10, 1, 0.1, 0.01, 0.001]\n",
    "\n",
    "metrics = []\n",
    "\n",
    "metrics.append(\n",
    "    {\n",
    "        'metric': LpMetric(p=1),\n",
    "        'name': 'L1'\n",
    "    }\n",
    ")\n",
    "\n",
    "metrics.append(\n",
    "    {\n",
    "        'metric': L2Metric(),\n",
    "        'name': 'L2'\n",
    "    }\n",
    ")\n",
    "metrics.append(\n",
    "    {\n",
    "        'metric': LinfMetric(),\n",
    "        'name': 'Linf'\n",
    "    }\n",
    ")\n",
    "\n",
    "# DSSIM\n",
    "for window_size in [5, 13, 21, 28]:\n",
    "    metrics.append(\n",
    "        {\n",
    "            'metric': StructuralDissimilarity(window_size=window_size),\n",
    "            'name': f'DSSIM_ws{window_size}'\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stani\\AppData\\Local\\Temp\\ipykernel_19000\\3669848702.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = metrics.sum() - c_lambda * loss_fn(model(adversarial_examples), torch.tensor(labels, dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n",
      "DONE metric L1\n",
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n",
      "DONE metric L2\n",
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n",
      "DONE metric Linf\n",
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n",
      "DONE metric DSSIM_ws5\n",
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n",
      "DONE metric DSSIM_ws13\n",
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n",
      "DONE metric DSSIM_ws21\n",
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n",
      "DONE metric DSSIM_ws28\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l2 = L2Metric()\n",
    "\n",
    "for metric in metrics:\n",
    "    for lambd in lambdas:\n",
    "        adv = cw_batch(model, benign, labels, lambd, metric['metric'])\n",
    "        metric['adv'] = adv\n",
    "        metric['success'] = torch.argmax(model(adv), dim=1) != labels\n",
    "        metric['dist'] = metric['metric'](benign, adv)\n",
    "        metric['L2_dist'] = l2(benign, adv)\n",
    "        for i, example in enumerate(adv):\n",
    "            ex = example.detach().reshape(28, 28)\n",
    "            plt.imshow(ex, cmap='gray', vmin=0, vmax=1)\n",
    "            plt.axis(\"off\")\n",
    "            plt.savefig(\n",
    "                f\"adversarials\\\\cw\\\\{metric['name']}_lambda{lambd}_{'adv' if metric['success'][i] else 'ben'}_dist{metric['dist'][i]}_d2{metric['L2_dist'][i]}_{i+1}.png\",\n",
    "                bbox_inches=\"tight\",\n",
    "                pad_inches=0)\n",
    "        print(f'___DONE lambda = {lambd}')\n",
    "    print(f\"DONE metric {metric['name']}\")\n",
    "\n",
    "for i, ben in enumerate(benign):\n",
    "    ex = ben.detach().reshape(28, 28)\n",
    "    plt.imshow(ex, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\n",
    "        f\"adversarials\\\\cw\\\\benign_{i+1}.png\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('adversarials\\\\cw\\\\results.json', 'w') as f:\n",
    "    metrics_to_json = []\n",
    "    for metric in metrics:\n",
    "        metrics_to_json.append(\n",
    "            {\n",
    "                'metric_name': metric['name'],\n",
    "                'success': metric['success'].tolist(),\n",
    "                'dist': metric['dist'].tolist(),\n",
    "                'L2_dist': metric['L2_dist'].tolist(),\n",
    "            }\n",
    "        )\n",
    "    json.dump(metrics_to_json, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stani\\AppData\\Local\\Temp\\ipykernel_19000\\100895427.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = metrics.sum() - c_lambda * loss_fn(model(adversarial_examples), torch.tensor(labels, dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for lambd in lambdas:\n",
    "    metric = {'metric': LinfMetric()}\n",
    "    metric['name'] = 'Linf_special'\n",
    "    adv = cw_batch(model, benign, labels, lambd, metric['metric'], special_init=True)\n",
    "    metric['adv'] = adv\n",
    "    metric['success'] = torch.argmax(model(adv), dim=1) != labels\n",
    "    metric['dist'] = metric['metric'](benign, adv)\n",
    "    metric['L2_dist'] = l2(benign, adv)\n",
    "    for i, example in enumerate(adv):\n",
    "        ex = example.detach().reshape(28, 28)\n",
    "        plt.imshow(ex, cmap='gray', vmin=0, vmax=1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.savefig(\n",
    "            f\"adversarials\\\\cw\\\\{metric['name']}_lambda{lambd}_{'adv' if metric['success'][i] else 'ben'}_dist{metric['dist'][i]}_d2{metric['L2_dist'][i]}_{i+1}.png\",\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0)\n",
    "    print(f'___DONE lambda = {lambd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('adversarials\\\\cw\\\\results.json', 'w') as f:\n",
    "    metrics_to_json = []\n",
    "    for metric in metrics:\n",
    "        metrics_to_json.append(\n",
    "            {\n",
    "                'metric_name': metric['name'],\n",
    "                'success': metric['success'].tolist(),\n",
    "                'dist': metric['dist'].tolist(),\n",
    "                'L2_dist': metric['L2_dist'].tolist(),\n",
    "            }\n",
    "        )\n",
    "    json.dump(metrics_to_json, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
