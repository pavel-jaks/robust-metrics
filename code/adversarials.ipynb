{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import Metric, WassersteinApproximation, StructuralDissimilarity, L2Metric, LinfNorm, LpMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models\\\\model_v1.model')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        'mnist',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    ),\n",
    "    batch_size=50,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_benign_examples(model, dataloader, count):\n",
    "    counter = 0\n",
    "    benign_examples = torch.zeros(count, 1, dataloader.dataset[0][0].shape[1], dataloader.dataset[0][0].shape[2])\n",
    "    benign_labels = torch.zeros(count)\n",
    "    for examples, labels in dataloader:\n",
    "        preds = model(examples)\n",
    "        match = (torch.argmax(preds, dim=1) == labels)\n",
    "        for idx, foo in enumerate(match):\n",
    "            if foo:\n",
    "                benign_examples[counter] = examples[idx]\n",
    "                benign_labels[counter] = labels[idx]\n",
    "                counter += 1\n",
    "            if counter >= count:\n",
    "                break\n",
    "        if counter >= count:\n",
    "            break\n",
    "    return benign_examples, benign_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 5\n",
    "\n",
    "benign, labels = get_benign_examples(model, train_loader, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cw_batch(model: nn.Module, benign_examples: torch.Tensor, labels: torch.Tensor, c_lambda: float, metric: Metric, special_init = False) -> torch.Tensor:\n",
    "    if special_init:\n",
    "        adversarial_examples = benign_examples\n",
    "    else:\n",
    "        adversarial_examples = 0.5 * torch.ones(benign_examples.shape) + 0.3 * (2 * torch.rand(benign_examples.shape) - 1)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    step_size = 1e-2\n",
    "    for i in range(100):\n",
    "        adversarial_examples.requires_grad = True\n",
    "        if adversarial_examples.grad is not None:\n",
    "            adversarial_examples.grad.zero_()\n",
    "        benign_examples.requires_grad = True\n",
    "        if benign_examples.grad is not None:\n",
    "            benign_examples.grad.zero_()\n",
    "        metrics = metric(benign_examples, adversarial_examples)\n",
    "        \n",
    "        loss = metrics[metrics == metrics].sum() - c_lambda * loss_fn(model(adversarial_examples), torch.tensor(labels, dtype=torch.long))\n",
    "        loss.backward()\n",
    "        adversarial_examples = (adversarial_examples - step_size * adversarial_examples.grad.apply_(lambda x: 1 if x >= 0 else -1)).detach()\n",
    "        adversarial_examples[adversarial_examples < 0] = 0\n",
    "        adversarial_examples[adversarial_examples > 1] = 1\n",
    "        \n",
    "    return adversarial_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [1000, 100, 10, 1, 0.1, 0.01, 0.001]\n",
    "\n",
    "metrics = []\n",
    "\n",
    "metrics.append(\n",
    "    {\n",
    "        'metric': LpMetric(p=1),\n",
    "        'name': 'L1'\n",
    "    }\n",
    ")\n",
    "\n",
    "metrics.append(\n",
    "    {\n",
    "        'metric': L2Metric(),\n",
    "        'name': 'L2'\n",
    "    }\n",
    ")\n",
    "\n",
    "# DSSIM\n",
    "for window_size in [5, 13, 21, 28]:\n",
    "    metrics.append(\n",
    "        {\n",
    "            'metric': StructuralDissimilarity(window_size=window_size),\n",
    "            'name': f'DSSIM_ws{window_size}'\n",
    "        }\n",
    "    )\n",
    "\n",
    "metrics.append(\n",
    "    {\n",
    "        'metric': WassersteinApproximation(regularization=3, iterations=150),\n",
    "        'name': 'WassersteinAproximation'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stani\\AppData\\Local\\Temp\\ipykernel_18224\\2774663418.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = metrics[metrics == metrics].sum() - c_lambda * loss_fn(model(adversarial_examples), torch.tensor(labels, dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n",
      "DONE metric L1\n",
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n",
      "DONE metric L2\n",
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n",
      "DONE metric DSSIM_ws5\n",
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n",
      "DONE metric DSSIM_ws13\n",
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n",
      "DONE metric DSSIM_ws21\n",
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n",
      "DONE metric DSSIM_ws28\n",
      "___DONE lambda = 1000\n",
      "___DONE lambda = 100\n",
      "___DONE lambda = 10\n",
      "___DONE lambda = 1\n",
      "___DONE lambda = 0.1\n",
      "___DONE lambda = 0.01\n",
      "___DONE lambda = 0.001\n",
      "DONE metric WassersteinAproximation\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l2 = L2Metric()\n",
    "\n",
    "for metric in metrics:\n",
    "    for lambd in lambdas:\n",
    "        adv = cw_batch(model, benign, labels, lambd, metric['metric'])\n",
    "        metric['adv'] = adv\n",
    "        metric['success'] = torch.argmax(model(adv), dim=1) != labels\n",
    "        metric['dist'] = metric['metric'](benign, adv)\n",
    "        metric['L2_dist'] = l2(benign, adv)\n",
    "        for i, example in enumerate(adv):\n",
    "            ex = example.detach().reshape(28, 28)\n",
    "            plt.imshow(ex, cmap='gray', vmin=0, vmax=1)\n",
    "            plt.axis(\"off\")\n",
    "            plt.savefig(\n",
    "                f\"adversarials\\\\test\\\\{metric['name']}_lambda={lambd}_{'adv' if metric['success'][i] else 'ben'}_dist={metric['dist'][i]}_d2={metric['L2_dist'][i]}_{i+1}.png\",\n",
    "                bbox_inches=\"tight\",\n",
    "                pad_inches=0)\n",
    "        print(f'___DONE lambda = {lambd}')\n",
    "    print(f\"DONE metric {metric['name']}\")\n",
    "\n",
    "for i, ben in enumerate(benign):\n",
    "    ex = ben.detach().reshape(28, 28)\n",
    "    plt.imshow(ex, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\n",
    "        f\"adversarials\\\\test\\\\benign_{i+1}.png\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('adversarials\\\\test\\\\results.json', 'w') as f:\n",
    "    metrics_to_json = []\n",
    "    for metric in metrics:\n",
    "        metrics_to_json.append(\n",
    "            {\n",
    "                'metric_name': metric['name'],\n",
    "                'success': metric['success'].tolist(),\n",
    "                'dist': metric['dist'].tolist(),\n",
    "                'L2_dist': metric['L2_dist'].tolist(),\n",
    "            }\n",
    "        )\n",
    "    json.dump(metrics_to_json, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
